{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb53b1a",
   "metadata": {},
   "source": [
    "# Decision Theory Project - TicTacToe\n",
    "*By Jelle Huibregtse and Aron Hemmes*\n",
    "\n",
    "Below is a TicTacToe environment build from scratch with an Agent based on reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e2276",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "## Loading in libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d535977f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# Libraries\\nimport random\\nimport functools\\nfrom enum import Enum\\nfrom IPython.display import display\\nfrom ipywidgets import Layout, Button, HTML, Box\\nfrom typing import List, Tuple\\nfrom copy import copy\\n\\n# Layout\\nfield_layout = Layout(width=\\\"50px\\\", height=\\\"50px\\\")\\nwide_layout = Layout(width=\\\"158px\\\")\\ncolumn_layout = Layout(flex_flow=\\\"column\\\")\\n\\n# Formatting\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# Libraries\\nimport random\\nimport functools\\nfrom enum import Enum\\nfrom IPython.display import display\\nfrom ipywidgets import Layout, Button, HTML, Box\\nfrom typing import List, Tuple\\nfrom copy import copy\\n\\n# Layout\\nfield_layout = Layout(width=\\\"50px\\\", height=\\\"50px\\\")\\nwide_layout = Layout(width=\\\"158px\\\")\\ncolumn_layout = Layout(flex_flow=\\\"column\\\")\\n\\n# Formatting\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Libraries\n",
    "import random\n",
    "import functools\n",
    "from enum import Enum\n",
    "from IPython.display import display\n",
    "from ipywidgets import Layout, Button, HTML, Box\n",
    "from typing import List, Tuple\n",
    "from copy import copy\n",
    "\n",
    "# Layout\n",
    "field_layout = Layout(width=\"50px\", height=\"50px\")\n",
    "wide_layout = Layout(width=\"158px\")\n",
    "column_layout = Layout(flex_flow=\"column\")\n",
    "\n",
    "# Formatting\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4857c9",
   "metadata": {},
   "source": [
    "## Configuring layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf218cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".widget-button {\n",
       "    outline: none !important;\n",
       "}\n",
       "\n",
       ".widget-html-content {\n",
       "    white-space: pre-wrap;\n",
       "    line-height: normal !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"%%HTML\\n<style>\\n.widget-button {\\n    outline: none !important;\\n}\\n\\n.widget-html-content {\\n    white-space: pre-wrap;\\n    line-height: normal !important;\\n}\\n</style>\";\n",
       "                var nbb_formatted_code = \"%%HTML\\n<style>\\n.widget-button {\\n    outline: none !important;\\n}\\n\\n.widget-html-content {\\n    white-space: pre-wrap;\\n    line-height: normal !important;\\n}\\n</style>\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".widget-button {\n",
    "    outline: none !important;\n",
    "}\n",
    "\n",
    ".widget-html-content {\n",
    "    white-space: pre-wrap;\n",
    "    line-height: normal !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9344c8bd",
   "metadata": {},
   "source": [
    "# 2. Definition of the Environment\n",
    "\n",
    "The code below defines all characteristics of our tic-tac-toe environment with the following characteristics:\n",
    "\n",
    "Environment state:\n",
    "\n",
    "- the player type is either X or O\n",
    "- the opposing player (agent) is either X or O depending on the player\n",
    "- X and O take turns placing an X or O on empty fields untill either one has won or there are no more fields left on the board\n",
    "- a board starts out empty and can contain X and O marks\n",
    "\n",
    "A TicTacToeEnvironment object has the following methods:\n",
    "- `reset()` which completely resets the board to an empty state.\n",
    "- `update()` Updates the visualisation of the current TicTacToe game.\n",
    "- `render()` Visualisation of the current TicTacToe game.\n",
    "- `change_player()` The player switches between X and O and resets the board.\n",
    "- `field_click(field)` The player sets a field to a itself if the field is None.\n",
    "\n",
    "Aditionally to allow an agent to calculate optimal decisions using model information, these methods are also available:\n",
    "- `get_turns()` Returns the amount of turns that have passed.\n",
    "- `get_active()` Returns which player's turn it currently is.\n",
    "- `get_state()` Returns a string representing the current board.\n",
    "- `get_reward(field)` Simplified version $R(s,a)$ of the general reward function: $R(s,â) = 1$, $R(s,ã) = -1$, $R(s,a) = 0$\n",
    "- `get_winnable_fields(player)` Returns all fields for a player that allow them to win. \n",
    "- `get_result()` Returns which player's got 3 in a row, otherwise returns None.\n",
    "- `set_field(field)` Sets a field to currently active player.\n",
    "- `step()` Processes the agent policies and returns the action, state and reward for the current active agent policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436da65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"rows = [\\n    [int(a), int(b), int(c)]\\n    for a, b, c in [\\\"036\\\", \\\"012\\\", \\\"147\\\", \\\"345\\\", \\\"048\\\", \\\"246\\\", \\\"678\\\", \\\"258\\\"]\\n]\\n\\n\\nclass Type(Enum):\\n    X = 1\\n    O = 2\\n\\n\\nclass TicTacToeEnvironment:\\n    def __init__(self, policies: list = None):\\n        self.board = [None for _ in range(9)]\\n        self.player = Type.X\\n        self.policies = policies\\n\\n        if not policies == None and not len(policies) > 1:\\n            self.step()\\n\\n    def get_turns(self) -> int:\\n        return len([field for field in self.board if not field == None])\\n\\n    def get_active(self) -> Type:\\n        return Type.X if self.get_turns() % 2 == 0 else Type.O\\n\\n    def get_state(self) -> str:\\n        return str([f.name if not f == None else \\\"E\\\" for f in self.board]).replace(\\n            \\\"'\\\", \\\"\\\"\\n        )\\n\\n    def get_result(self) -> Type:\\n        # Check for three of the same marks in a row\\n        board = self.board\\n        for row in rows:\\n            # If not none and three in a row are the same\\n            if (\\n                not board[row[0]] == None\\n                and board[row[0]] == board[row[1]] == board[row[2]]\\n            ):\\n                # Return board\\n                return board[row[0]]\\n\\n    def step(self) -> Tuple[Type, int, float, str]:\\n        # Execute agent code\\n        field = None\\n        if self.get_result() == None:\\n            if not self.policies == None:\\n                if len(self.policies) == 1:\\n                    if not self.player == self.get_active():\\n                        field = self.policies[0](self)\\n                if len(self.policies) == 2:\\n                    field = self.policies[0 if self.player == self.get_active() else 1](\\n                        self\\n                    )\\n\\n        if not field == None:\\n            player, reward = self.get_active(), self.get_reward(field)\\n            self.set_field(field)\\n            return player, field, reward, self.get_state()\\n\\n    def get_reward(self, field: int) -> float:\\n        player_fields = self.get_winnable_fields(self.get_active())\\n        opposing_fields = self.get_winnable_fields(\\n            Type.O if self.get_active() == Type.X else Type.X\\n        )\\n        # If player places in field with 3 in a row, return 1.0\\n        if field in player_fields:\\n            return 1.0\\n        # If player doesn't place in field where opposing can get 3 in a row, return -1.0\\n        elif len(opposing_fields) > 0 and not field in opposing_fields:\\n            return -1.0\\n        # Return 0.0 by default\\n        return 0.0\\n\\n    def reset(self) -> None:\\n        self.board = [None for _ in range(9)]\\n        self.update()\\n\\n        if not self.policies == None and not len(self.policies) > 1:\\n            self.step()\\n\\n    def set_field(self, field: int) -> None:\\n        # Set field to current\\n        self.board[field] = self.get_active()\\n\\n        # Update the board\\n        self.update()\\n\\n    def get_winnable_fields(self, player: Type) -> List[int]:\\n        # If there are 2 matching player type and 1 matching None return the fields\\n        result = []\\n        for row in rows:\\n            fields = [[c, self.board[c]] for c in row]\\n            noneMatches = [f for f in fields if f[1] == None]\\n            playerMatches = [f for f in fields if f[1] == player]\\n            if len(noneMatches) == 1 and len(playerMatches) == 2:\\n                result.append(noneMatches[0][0])\\n\\n        return result\\n\\n    def change_player(self) -> None:\\n        self.player = Type.O if self.player == Type.X else Type.X\\n        self.reset()\\n\\n    # ----------------------------\\n    # Visual board methods\\n    # ----------------------------\\n    def field_click(self, field: int) -> None:\\n        if self.policies == None or not len(self.policies) > 1:\\n            if self.get_result() == None and self.board[field] == None:\\n                if self.policies == None or self.get_active() == self.player:\\n                    self.set_field(field)\\n                if not self.policies == None and not self.get_active() == self.player:\\n                    self.step()\\n\\n    def update(self) -> None:\\n        if hasattr(self, \\\"field_buttons\\\"):\\n            for i in range(len(self.field_buttons)):\\n                self.field_buttons[i].description = (\\n                    self.board[i].name if not self.board[i] == None else \\\" \\\"\\n                )\\n            if hasattr(self, \\\"player_select_button\\\"):\\n                self.player_select_button.description = \\\"PLAYER \\\" + self.player.name\\n            result = self.get_result()\\n            if not result == None:\\n                self.result_text.value = \\\"winner is <b>{}</b>\\\".format(result.name)\\n            else:\\n                self.result_text.value = \\\"\\\"\\n\\n    def render(self) -> None:\\n        if not hasattr(self, \\\"field_buttons\\\"):\\n            self.field_buttons = []\\n            elements = []\\n\\n            # Add header\\n            if not self.policies == None and not len(self.policies) > 1:\\n                elements.append(HTML(value=\\\"<h1>TicTacToe</h1>\\\"))\\n\\n            # Add field buttons\\n            buttons = []\\n            rows = []\\n            for i in range(9):\\n                btn = Button(layout=field_layout)\\n                btn.on_click(functools.partial(self.field_click, i))\\n                buttons.append(btn)\\n                self.field_buttons.append(btn)\\n                if (i + 1) % 3 == 0:\\n                    rows.append(Box(buttons))\\n                    buttons = []\\n            elements.append(Box(children=rows, layout=column_layout))\\n\\n            # Add player select\\n            if not self.policies == None and not len(self.policies) > 1:\\n                player_btn = Button(\\n                    description=\\\"PLAYER \\\" + self.player.name, layout=wide_layout\\n                )\\n                player_btn.on_click(self.change_player)\\n                self.player_select_button = player_btn\\n\\n                elements.append(player_btn)\\n\\n            # Add reset button\\n            if not self.policies == None and not len(self.policies) > 1:\\n                reset_btn = Button(description=\\\"RESET\\\", layout=wide_layout)\\n                reset_btn.on_click(self.reset)\\n                elements.append(reset_btn)\\n\\n            # Add result text\\n            result_text = HTML()\\n            elements.append(result_text)\\n            self.result_text = result_text\\n\\n            # Display elements and data\\n            display(Box(children=elements, layout=column_layout))\\n\\n        # Update the board\\n        self.update()\";\n",
       "                var nbb_formatted_code = \"rows = [\\n    [int(a), int(b), int(c)]\\n    for a, b, c in [\\\"036\\\", \\\"012\\\", \\\"147\\\", \\\"345\\\", \\\"048\\\", \\\"246\\\", \\\"678\\\", \\\"258\\\"]\\n]\\n\\n\\nclass Type(Enum):\\n    X = 1\\n    O = 2\\n\\n\\nclass TicTacToeEnvironment:\\n    def __init__(self, policies: list = None):\\n        self.board = [None for _ in range(9)]\\n        self.player = Type.X\\n        self.policies = policies\\n\\n        if not policies == None and not len(policies) > 1:\\n            self.step()\\n\\n    def get_turns(self) -> int:\\n        return len([field for field in self.board if not field == None])\\n\\n    def get_active(self) -> Type:\\n        return Type.X if self.get_turns() % 2 == 0 else Type.O\\n\\n    def get_state(self) -> str:\\n        return str([f.name if not f == None else \\\"E\\\" for f in self.board]).replace(\\n            \\\"'\\\", \\\"\\\"\\n        )\\n\\n    def get_result(self) -> Type:\\n        # Check for three of the same marks in a row\\n        board = self.board\\n        for row in rows:\\n            # If not none and three in a row are the same\\n            if (\\n                not board[row[0]] == None\\n                and board[row[0]] == board[row[1]] == board[row[2]]\\n            ):\\n                # Return board\\n                return board[row[0]]\\n\\n    def step(self) -> Tuple[Type, int, float, str]:\\n        # Execute agent code\\n        field = None\\n        if self.get_result() == None:\\n            if not self.policies == None:\\n                if len(self.policies) == 1:\\n                    if not self.player == self.get_active():\\n                        field = self.policies[0](self)\\n                if len(self.policies) == 2:\\n                    field = self.policies[0 if self.player == self.get_active() else 1](\\n                        self\\n                    )\\n\\n        if not field == None:\\n            player, reward = self.get_active(), self.get_reward(field)\\n            self.set_field(field)\\n            return player, field, reward, self.get_state()\\n\\n    def get_reward(self, field: int) -> float:\\n        player_fields = self.get_winnable_fields(self.get_active())\\n        opposing_fields = self.get_winnable_fields(\\n            Type.O if self.get_active() == Type.X else Type.X\\n        )\\n        # If player places in field with 3 in a row, return 1.0\\n        if field in player_fields:\\n            return 1.0\\n        # If player doesn't place in field where opposing can get 3 in a row, return -1.0\\n        elif len(opposing_fields) > 0 and not field in opposing_fields:\\n            return -1.0\\n        # Return 0.0 by default\\n        return 0.0\\n\\n    def reset(self) -> None:\\n        self.board = [None for _ in range(9)]\\n        self.update()\\n\\n        if not self.policies == None and not len(self.policies) > 1:\\n            self.step()\\n\\n    def set_field(self, field: int) -> None:\\n        # Set field to current\\n        self.board[field] = self.get_active()\\n\\n        # Update the board\\n        self.update()\\n\\n    def get_winnable_fields(self, player: Type) -> List[int]:\\n        # If there are 2 matching player type and 1 matching None return the fields\\n        result = []\\n        for row in rows:\\n            fields = [[c, self.board[c]] for c in row]\\n            noneMatches = [f for f in fields if f[1] == None]\\n            playerMatches = [f for f in fields if f[1] == player]\\n            if len(noneMatches) == 1 and len(playerMatches) == 2:\\n                result.append(noneMatches[0][0])\\n\\n        return result\\n\\n    def change_player(self) -> None:\\n        self.player = Type.O if self.player == Type.X else Type.X\\n        self.reset()\\n\\n    # ----------------------------\\n    # Visual board methods\\n    # ----------------------------\\n    def field_click(self, field: int) -> None:\\n        if self.policies == None or not len(self.policies) > 1:\\n            if self.get_result() == None and self.board[field] == None:\\n                if self.policies == None or self.get_active() == self.player:\\n                    self.set_field(field)\\n                if not self.policies == None and not self.get_active() == self.player:\\n                    self.step()\\n\\n    def update(self) -> None:\\n        if hasattr(self, \\\"field_buttons\\\"):\\n            for i in range(len(self.field_buttons)):\\n                self.field_buttons[i].description = (\\n                    self.board[i].name if not self.board[i] == None else \\\" \\\"\\n                )\\n            if hasattr(self, \\\"player_select_button\\\"):\\n                self.player_select_button.description = \\\"PLAYER \\\" + self.player.name\\n            result = self.get_result()\\n            if not result == None:\\n                self.result_text.value = \\\"winner is <b>{}</b>\\\".format(result.name)\\n            else:\\n                self.result_text.value = \\\"\\\"\\n\\n    def render(self) -> None:\\n        if not hasattr(self, \\\"field_buttons\\\"):\\n            self.field_buttons = []\\n            elements = []\\n\\n            # Add header\\n            if not self.policies == None and not len(self.policies) > 1:\\n                elements.append(HTML(value=\\\"<h1>TicTacToe</h1>\\\"))\\n\\n            # Add field buttons\\n            buttons = []\\n            rows = []\\n            for i in range(9):\\n                btn = Button(layout=field_layout)\\n                btn.on_click(functools.partial(self.field_click, i))\\n                buttons.append(btn)\\n                self.field_buttons.append(btn)\\n                if (i + 1) % 3 == 0:\\n                    rows.append(Box(buttons))\\n                    buttons = []\\n            elements.append(Box(children=rows, layout=column_layout))\\n\\n            # Add player select\\n            if not self.policies == None and not len(self.policies) > 1:\\n                player_btn = Button(\\n                    description=\\\"PLAYER \\\" + self.player.name, layout=wide_layout\\n                )\\n                player_btn.on_click(self.change_player)\\n                self.player_select_button = player_btn\\n\\n                elements.append(player_btn)\\n\\n            # Add reset button\\n            if not self.policies == None and not len(self.policies) > 1:\\n                reset_btn = Button(description=\\\"RESET\\\", layout=wide_layout)\\n                reset_btn.on_click(self.reset)\\n                elements.append(reset_btn)\\n\\n            # Add result text\\n            result_text = HTML()\\n            elements.append(result_text)\\n            self.result_text = result_text\\n\\n            # Display elements and data\\n            display(Box(children=elements, layout=column_layout))\\n\\n        # Update the board\\n        self.update()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = [\n",
    "    [int(a), int(b), int(c)]\n",
    "    for a, b, c in [\"036\", \"012\", \"147\", \"345\", \"048\", \"246\", \"678\", \"258\"]\n",
    "]\n",
    "\n",
    "\n",
    "class Type(Enum):\n",
    "    X = 1\n",
    "    O = 2\n",
    "\n",
    "\n",
    "class TicTacToeEnvironment:\n",
    "    def __init__(self, policies: list = None):\n",
    "        self.board = [None for _ in range(9)]\n",
    "        self.player = Type.X\n",
    "        self.policies = policies\n",
    "\n",
    "        if not policies == None and not len(policies) > 1:\n",
    "            self.step()\n",
    "\n",
    "    def get_turns(self) -> int:\n",
    "        return len([field for field in self.board if not field == None])\n",
    "\n",
    "    def get_active(self) -> Type:\n",
    "        return Type.X if self.get_turns() % 2 == 0 else Type.O\n",
    "\n",
    "    def get_state(self) -> str:\n",
    "        return str([f.name if not f == None else \"E\" for f in self.board]).replace(\n",
    "            \"'\", \"\"\n",
    "        )\n",
    "\n",
    "    def get_result(self) -> Type:\n",
    "        # Check for three of the same marks in a row\n",
    "        board = self.board\n",
    "        for row in rows:\n",
    "            # If not none and three in a row are the same\n",
    "            if (\n",
    "                not board[row[0]] == None\n",
    "                and board[row[0]] == board[row[1]] == board[row[2]]\n",
    "            ):\n",
    "                # Return board\n",
    "                return board[row[0]]\n",
    "\n",
    "    def step(self) -> Tuple[Type, int, float, str]:\n",
    "        # Execute agent code\n",
    "        field = None\n",
    "        if self.get_result() == None:\n",
    "            if not self.policies == None:\n",
    "                if len(self.policies) == 1:\n",
    "                    if not self.player == self.get_active():\n",
    "                        field = self.policies[0](self)\n",
    "                if len(self.policies) == 2:\n",
    "                    field = self.policies[0 if self.player == self.get_active() else 1](\n",
    "                        self\n",
    "                    )\n",
    "\n",
    "        if not field == None:\n",
    "            player, reward = self.get_active(), self.get_reward(field)\n",
    "            self.set_field(field)\n",
    "            return player, field, reward, self.get_state()\n",
    "\n",
    "    def get_reward(self, field: int) -> float:\n",
    "        player_fields = self.get_winnable_fields(self.get_active())\n",
    "        opposing_fields = self.get_winnable_fields(\n",
    "            Type.O if self.get_active() == Type.X else Type.X\n",
    "        )\n",
    "        # If player places in field with 3 in a row, return 1.0\n",
    "        if field in player_fields:\n",
    "            return 1.0\n",
    "        # If player doesn't place in field where opposing can get 3 in a row, return -1.0\n",
    "        elif len(opposing_fields) > 0 and not field in opposing_fields:\n",
    "            return -1.0\n",
    "        # Return 0.0 by default\n",
    "        return 0.0\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.board = [None for _ in range(9)]\n",
    "        self.update()\n",
    "\n",
    "        if not self.policies == None and not len(self.policies) > 1:\n",
    "            self.step()\n",
    "\n",
    "    def set_field(self, field: int) -> None:\n",
    "        # Set field to current\n",
    "        self.board[field] = self.get_active()\n",
    "\n",
    "        # Update the board\n",
    "        self.update()\n",
    "\n",
    "    def get_winnable_fields(self, player: Type) -> List[int]:\n",
    "        # If there are 2 matching player type and 1 matching None return the fields\n",
    "        result = []\n",
    "        for row in rows:\n",
    "            fields = [[c, self.board[c]] for c in row]\n",
    "            noneMatches = [f for f in fields if f[1] == None]\n",
    "            playerMatches = [f for f in fields if f[1] == player]\n",
    "            if len(noneMatches) == 1 and len(playerMatches) == 2:\n",
    "                result.append(noneMatches[0][0])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def change_player(self) -> None:\n",
    "        self.player = Type.O if self.player == Type.X else Type.X\n",
    "        self.reset()\n",
    "\n",
    "    # ----------------------------\n",
    "    # Visual board methods\n",
    "    # ----------------------------\n",
    "    def field_click(self, field: int) -> None:\n",
    "        if self.policies == None or not len(self.policies) > 1:\n",
    "            if self.get_result() == None and self.board[field] == None:\n",
    "                if self.policies == None or self.get_active() == self.player:\n",
    "                    self.set_field(field)\n",
    "                if not self.policies == None and not self.get_active() == self.player:\n",
    "                    self.step()\n",
    "\n",
    "    def update(self) -> None:\n",
    "        if hasattr(self, \"field_buttons\"):\n",
    "            for i in range(len(self.field_buttons)):\n",
    "                self.field_buttons[i].description = (\n",
    "                    self.board[i].name if not self.board[i] == None else \" \"\n",
    "                )\n",
    "            if hasattr(self, \"player_select_button\"):\n",
    "                self.player_select_button.description = \"PLAYER \" + self.player.name\n",
    "            result = self.get_result()\n",
    "            if not result == None:\n",
    "                self.result_text.value = \"winner is <b>{}</b>\".format(result.name)\n",
    "            else:\n",
    "                self.result_text.value = \"\"\n",
    "\n",
    "    def render(self) -> None:\n",
    "        if not hasattr(self, \"field_buttons\"):\n",
    "            self.field_buttons = []\n",
    "            elements = []\n",
    "\n",
    "            # Add header\n",
    "            if not self.policies == None and not len(self.policies) > 1:\n",
    "                elements.append(HTML(value=\"<h1>TicTacToe</h1>\"))\n",
    "\n",
    "            # Add field buttons\n",
    "            buttons = []\n",
    "            rows = []\n",
    "            for i in range(9):\n",
    "                btn = Button(layout=field_layout)\n",
    "                btn.on_click(functools.partial(self.field_click, i))\n",
    "                buttons.append(btn)\n",
    "                self.field_buttons.append(btn)\n",
    "                if (i + 1) % 3 == 0:\n",
    "                    rows.append(Box(buttons))\n",
    "                    buttons = []\n",
    "            elements.append(Box(children=rows, layout=column_layout))\n",
    "\n",
    "            # Add player select\n",
    "            if not self.policies == None and not len(self.policies) > 1:\n",
    "                player_btn = Button(\n",
    "                    description=\"PLAYER \" + self.player.name, layout=wide_layout\n",
    "                )\n",
    "                player_btn.on_click(self.change_player)\n",
    "                self.player_select_button = player_btn\n",
    "\n",
    "                elements.append(player_btn)\n",
    "\n",
    "            # Add reset button\n",
    "            if not self.policies == None and not len(self.policies) > 1:\n",
    "                reset_btn = Button(description=\"RESET\", layout=wide_layout)\n",
    "                reset_btn.on_click(self.reset)\n",
    "                elements.append(reset_btn)\n",
    "\n",
    "            # Add result text\n",
    "            result_text = HTML()\n",
    "            elements.append(result_text)\n",
    "            self.result_text = result_text\n",
    "\n",
    "            # Display elements and data\n",
    "            display(Box(children=elements, layout=column_layout))\n",
    "\n",
    "        # Update the board\n",
    "        self.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ed233c",
   "metadata": {},
   "source": [
    "## Creation of an Environment\n",
    "The `TicTacToeEnvironment` class allows the creation of an tic-tac-toe environment that can take in different policies as parameters. An example is below with no agent (no policy) and player vs. player. Note that game is interactable. Furthermore, this is the default state of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b7036b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e25ccfae9be4014b6e6f9531deb75e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Box(children=(Box(children=(Button(layout=Layout(height='50px', width='50px'), style=ButtonStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# Initializing environment and rendering\\ntictactoe = TicTacToeEnvironment()\\ntictactoe.render()\";\n",
       "                var nbb_formatted_code = \"# Initializing environment and rendering\\ntictactoe = TicTacToeEnvironment()\\ntictactoe.render()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initializing environment and rendering\n",
    "tictactoe = TicTacToeEnvironment()\n",
    "tictactoe.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636121d1",
   "metadata": {},
   "source": [
    "## Action Space\n",
    "\n",
    "The actions space of the tic-tac-toe environment is placing a `Type` on one of the nine fields. The `Type` enum is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a7fe3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action 1 is Type.X\n",
      "action 2 is Type.O\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"for nr, action in enumerate(Type, 1):\\n    print(\\\"action\\\", nr, \\\"is\\\", action)\";\n",
       "                var nbb_formatted_code = \"for nr, action in enumerate(Type, 1):\\n    print(\\\"action\\\", nr, \\\"is\\\", action)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for nr, action in enumerate(Type, 1):\n",
    "    print(\"action\", nr, \"is\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a7f85",
   "metadata": {},
   "source": [
    "## Transitions\n",
    "Usually, we also define a transition function $T(s,a,s')$ that gives the probability of moving from a state $s$ to $s'$ when performing action $a$. This is only used in an environment where we are unsure or where it is unclear if some action always gives a predictable outcome. However, since tic-tac-toe is a simple game where every action is defined, we won't be needing a transition function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610cad75",
   "metadata": {},
   "source": [
    "# 3. Unbeatable computer\n",
    "\n",
    "An unbeatable computer that consistently plays the same best moves, sometimes there are multiple equally good moves, then it takes the first one of these moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9146b50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba02c3585e974ac7bce73e5598f13dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(HTML(value='<h1>TicTacToe</h1>'), Box(children=(Box(children=(Button(layout=Layout(height='50px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"def computed_best_policy(tictactoe: TicTacToeEnvironment) -> int:\\n    \\\"\\\"\\\"Return field computed with if statements\\\"\\\"\\\"\\n    player = tictactoe.get_active()\\n    opponent = Type.X if player == Type.O else Type.O\\n\\n    # If player is able to make 3 in a row, place to win\\n    fields = tictactoe.get_winnable_fields(player)\\n    if len(fields) > 0:\\n        return fields[0]\\n\\n    # If opponent is able to make 3 in a row, place to prevent\\n    fields = tictactoe.get_winnable_fields(opponent)\\n    if len(fields) > 0:\\n        return fields[0]\\n\\n    # If center is empty, place in center\\n    if tictactoe.board[4] == None:\\n        return 4\\n\\n    # If a corner is empty and a corner on the opposite diagonal side is also empty, place in this first empty corner\\n    empty_diagonal_corners = [\\n        i[0]\\n        for i in [[0, 8], [2, 6]]\\n        if tictactoe.board[i[0]] == None and tictactoe.board[i[1]] == None\\n    ]\\n    if len(empty_diagonal_corners) > 0:\\n        return empty_diagonal_corners[0]\\n\\n    # If a corner is empty, place in first empty corner\\n    empty_corners = [i for i in [0, 2, 6, 8] if tictactoe.board[i] == None]\\n    if len(empty_corners) > 0:\\n        return empty_corners[0]\\n\\n    # Place in first empty field\\n    empty_fields = [\\n        [i, tictactoe.board[i]]\\n        for i in range(len(tictactoe.board))\\n        if tictactoe.board[i] == None\\n    ]\\n    if len(empty_fields) > 0:\\n        return empty_fields[0][0]\\n\\n\\n# Initializing environment and rendering\\ntictactoe = TicTacToeEnvironment([computed_best_policy])\\ntictactoe.render()\";\n",
       "                var nbb_formatted_code = \"def computed_best_policy(tictactoe: TicTacToeEnvironment) -> int:\\n    \\\"\\\"\\\"Return field computed with if statements\\\"\\\"\\\"\\n    player = tictactoe.get_active()\\n    opponent = Type.X if player == Type.O else Type.O\\n\\n    # If player is able to make 3 in a row, place to win\\n    fields = tictactoe.get_winnable_fields(player)\\n    if len(fields) > 0:\\n        return fields[0]\\n\\n    # If opponent is able to make 3 in a row, place to prevent\\n    fields = tictactoe.get_winnable_fields(opponent)\\n    if len(fields) > 0:\\n        return fields[0]\\n\\n    # If center is empty, place in center\\n    if tictactoe.board[4] == None:\\n        return 4\\n\\n    # If a corner is empty and a corner on the opposite diagonal side is also empty, place in this first empty corner\\n    empty_diagonal_corners = [\\n        i[0]\\n        for i in [[0, 8], [2, 6]]\\n        if tictactoe.board[i[0]] == None and tictactoe.board[i[1]] == None\\n    ]\\n    if len(empty_diagonal_corners) > 0:\\n        return empty_diagonal_corners[0]\\n\\n    # If a corner is empty, place in first empty corner\\n    empty_corners = [i for i in [0, 2, 6, 8] if tictactoe.board[i] == None]\\n    if len(empty_corners) > 0:\\n        return empty_corners[0]\\n\\n    # Place in first empty field\\n    empty_fields = [\\n        [i, tictactoe.board[i]]\\n        for i in range(len(tictactoe.board))\\n        if tictactoe.board[i] == None\\n    ]\\n    if len(empty_fields) > 0:\\n        return empty_fields[0][0]\\n\\n\\n# Initializing environment and rendering\\ntictactoe = TicTacToeEnvironment([computed_best_policy])\\ntictactoe.render()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def computed_best_policy(tictactoe: TicTacToeEnvironment) -> int:\n",
    "    \"\"\"Return field computed with if statements\"\"\"\n",
    "    player = tictactoe.get_active()\n",
    "    opponent = Type.X if player == Type.O else Type.O\n",
    "\n",
    "    # If player is able to make 3 in a row, place to win\n",
    "    fields = tictactoe.get_winnable_fields(player)\n",
    "    if len(fields) > 0:\n",
    "        return fields[0]\n",
    "\n",
    "    # If opponent is able to make 3 in a row, place to prevent\n",
    "    fields = tictactoe.get_winnable_fields(opponent)\n",
    "    if len(fields) > 0:\n",
    "        return fields[0]\n",
    "\n",
    "    # If center is empty, place in center\n",
    "    if tictactoe.board[4] == None:\n",
    "        return 4\n",
    "\n",
    "    # If a corner is empty and a corner on the opposite diagonal side is also empty, place in this first empty corner\n",
    "    empty_diagonal_corners = [\n",
    "        i[0]\n",
    "        for i in [[0, 8], [2, 6]]\n",
    "        if tictactoe.board[i[0]] == None and tictactoe.board[i[1]] == None\n",
    "    ]\n",
    "    if len(empty_diagonal_corners) > 0:\n",
    "        return empty_diagonal_corners[0]\n",
    "\n",
    "    # If a corner is empty, place in first empty corner\n",
    "    empty_corners = [i for i in [0, 2, 6, 8] if tictactoe.board[i] == None]\n",
    "    if len(empty_corners) > 0:\n",
    "        return empty_corners[0]\n",
    "\n",
    "    # Place in first empty field\n",
    "    empty_fields = [\n",
    "        [i, tictactoe.board[i]]\n",
    "        for i in range(len(tictactoe.board))\n",
    "        if tictactoe.board[i] == None\n",
    "    ]\n",
    "    if len(empty_fields) > 0:\n",
    "        return empty_fields[0][0]\n",
    "\n",
    "\n",
    "# Initializing environment and rendering\n",
    "tictactoe = TicTacToeEnvironment([computed_best_policy])\n",
    "tictactoe.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a8aa0",
   "metadata": {},
   "source": [
    "# 4. Random Agent\n",
    "\n",
    "Two agents play against each other: \n",
    "\n",
    "- Agent X: Uses the `computed_best_policy`.\n",
    "- Agent O: Uses the `random_policy`.\n",
    "\n",
    "In the cell below, you can see the effect of an agent with a random policy choosing an arbitrary action regardless of the new state, playing against the `computed_best_policy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a11aef49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: X 4\tstate: [E, E, E, E, X, E, E, E, E]\n",
      "action: O 0\tstate: [O, E, E, E, X, E, E, E, E], reward: 0.0\n",
      "action: X 2\tstate: [O, E, X, E, X, E, E, E, E]\n",
      "action: O 3\tstate: [O, E, X, O, X, E, E, E, E], reward: -1.0\n",
      "action: X 6\tstate: [O, E, X, O, X, E, X, E, E]\n",
      "\n",
      "Episode done after 5 steps. Total reward: -1.0\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"def random_policy(tictactoe: TicTacToeEnvironment) -> int:\\n    \\\"\\\"\\\"Return random field in empty fields\\\"\\\"\\\"\\n\\n    # Getting all the empty fields\\n    empty_fields = [f for f in range(9) if tictactoe.board[f] == None]\\n\\n    # Choose random empty field\\n    if len(empty_fields) > 0:\\n        return random.choice(empty_fields)\\n\\n\\ntotal_reward = 0.0\\n\\ntictactoe = TicTacToeEnvironment([computed_best_policy, random_policy])\\nwhile (\\n    tictactoe.get_result() == None\\n    and len([f for f in tictactoe.board if f == None]) > 0\\n):\\n    player, field, reward, state = tictactoe.step()\\n    if player == Type.O:\\n        total_reward += reward\\n        print(\\n            \\\"action: {}\\\\tstate: {}, reward: {}\\\".format(\\n                player.name + \\\" \\\" + str(field), state, reward\\n            )\\n        )\\n    else:\\n        print(\\\"action: {}\\\\tstate: {}\\\".format(player.name + \\\" \\\" + str(field), state))\\nprint(\\n    \\\"\\\\nEpisode done after {} steps. Total reward: {}\\\".format(\\n        tictactoe.get_turns(), total_reward\\n    )\\n)\";\n",
       "                var nbb_formatted_code = \"def random_policy(tictactoe: TicTacToeEnvironment) -> int:\\n    \\\"\\\"\\\"Return random field in empty fields\\\"\\\"\\\"\\n\\n    # Getting all the empty fields\\n    empty_fields = [f for f in range(9) if tictactoe.board[f] == None]\\n\\n    # Choose random empty field\\n    if len(empty_fields) > 0:\\n        return random.choice(empty_fields)\\n\\n\\ntotal_reward = 0.0\\n\\ntictactoe = TicTacToeEnvironment([computed_best_policy, random_policy])\\nwhile (\\n    tictactoe.get_result() == None\\n    and len([f for f in tictactoe.board if f == None]) > 0\\n):\\n    player, field, reward, state = tictactoe.step()\\n    if player == Type.O:\\n        total_reward += reward\\n        print(\\n            \\\"action: {}\\\\tstate: {}, reward: {}\\\".format(\\n                player.name + \\\" \\\" + str(field), state, reward\\n            )\\n        )\\n    else:\\n        print(\\\"action: {}\\\\tstate: {}\\\".format(player.name + \\\" \\\" + str(field), state))\\nprint(\\n    \\\"\\\\nEpisode done after {} steps. Total reward: {}\\\".format(\\n        tictactoe.get_turns(), total_reward\\n    )\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def random_policy(tictactoe: TicTacToeEnvironment) -> int:\n",
    "    \"\"\"Return random field in empty fields\"\"\"\n",
    "\n",
    "    # Getting all the empty fields\n",
    "    empty_fields = [f for f in range(9) if tictactoe.board[f] == None]\n",
    "\n",
    "    # Choose random empty field\n",
    "    if len(empty_fields) > 0:\n",
    "        return random.choice(empty_fields)\n",
    "\n",
    "\n",
    "total_reward = 0.0\n",
    "\n",
    "tictactoe = TicTacToeEnvironment([computed_best_policy, random_policy])\n",
    "while (\n",
    "    tictactoe.get_result() == None\n",
    "    and len([f for f in tictactoe.board if f == None]) > 0\n",
    "):\n",
    "    player, field, reward, state = tictactoe.step()\n",
    "    if player == Type.O:\n",
    "        total_reward += reward\n",
    "        print(\n",
    "            \"action: {}\\tstate: {}, reward: {}\".format(\n",
    "                player.name + \" \" + str(field), state, reward\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\"action: {}\\tstate: {}\".format(player.name + \" \" + str(field), state))\n",
    "print(\n",
    "    \"\\nEpisode done after {} steps. Total reward: {}\".format(\n",
    "        tictactoe.get_turns(), total_reward\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c4c3c",
   "metadata": {},
   "source": [
    "If you run the cell above a number of times, you can observe two things:\n",
    "- The total reward and steps will vary from run to run.\n",
    "- O never wins, but sometimes the game ends in a tie.\n",
    "\n",
    "Each run from start state until stop state is called an episode.  \n",
    "Let's assemble some statistics on the episodes of the random agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7b2bceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics over 100 episodes\n",
      "mean:  -0.96, sigma:   0.20\n",
      "\n",
      "ep:  1, total reward: -1.00\n",
      "ep:  2, total reward: -1.00\n",
      "ep:  3, total reward: -1.00\n",
      "ep:  4, total reward: -1.00\n",
      "ep:  5, total reward: -1.00\n",
      "......\n",
      "ep: 95, total reward: -1.00\n",
      "ep: 96, total reward: -1.00\n",
      "ep: 97, total reward: -1.00\n",
      "ep: 98, total reward: -1.00\n",
      "ep: 99, total reward: -1.00\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"from statistics import mean, stdev\\n\\n\\ndef run_one_episode(policy) -> int:\\n    tictactoe = TicTacToeEnvironment([computed_best_policy, policy])\\n    total_reward = 0.0\\n    while (\\n        tictactoe.get_result() == None\\n        and len([f for f in tictactoe.board if f == None]) > 0\\n    ):\\n        player, field, reward, state = tictactoe.step()\\n        if player == Type.O:\\n            total_reward += reward\\n    return total_reward\\n\\n\\ndef measure_performance(policy, nr_episodes: int = 100):\\n    N = nr_episodes\\n    print(\\\"statistics over\\\", N, \\\"episodes\\\")\\n    all_rewards = []\\n    for _ in range(N):\\n        episode_reward = run_one_episode(policy)\\n        all_rewards.append(episode_reward)\\n\\n    print(\\\"mean: {:6.2f}, sigma: {:6.2f}\\\".format(mean(all_rewards), stdev(all_rewards)))\\n    print()\\n    for n, episode_reward in enumerate(all_rewards[:5], 1):\\n        print(\\\"ep: {:2d}, total reward: {:5.2f}\\\".format(n, episode_reward))\\n    print(\\\"......\\\")\\n    for n, episode_reward in enumerate(all_rewards[-5:], len(all_rewards) - 5):\\n        print(\\\"ep: {:2d}, total reward: {:5.2f}\\\".format(n, episode_reward))\\n\\n\\nmeasure_performance(random_policy)\";\n",
       "                var nbb_formatted_code = \"from statistics import mean, stdev\\n\\n\\ndef run_one_episode(policy) -> int:\\n    tictactoe = TicTacToeEnvironment([computed_best_policy, policy])\\n    total_reward = 0.0\\n    while (\\n        tictactoe.get_result() == None\\n        and len([f for f in tictactoe.board if f == None]) > 0\\n    ):\\n        player, field, reward, state = tictactoe.step()\\n        if player == Type.O:\\n            total_reward += reward\\n    return total_reward\\n\\n\\ndef measure_performance(policy, nr_episodes: int = 100):\\n    N = nr_episodes\\n    print(\\\"statistics over\\\", N, \\\"episodes\\\")\\n    all_rewards = []\\n    for _ in range(N):\\n        episode_reward = run_one_episode(policy)\\n        all_rewards.append(episode_reward)\\n\\n    print(\\\"mean: {:6.2f}, sigma: {:6.2f}\\\".format(mean(all_rewards), stdev(all_rewards)))\\n    print()\\n    for n, episode_reward in enumerate(all_rewards[:5], 1):\\n        print(\\\"ep: {:2d}, total reward: {:5.2f}\\\".format(n, episode_reward))\\n    print(\\\"......\\\")\\n    for n, episode_reward in enumerate(all_rewards[-5:], len(all_rewards) - 5):\\n        print(\\\"ep: {:2d}, total reward: {:5.2f}\\\".format(n, episode_reward))\\n\\n\\nmeasure_performance(random_policy)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "\n",
    "\n",
    "def run_one_episode(policy) -> int:\n",
    "    tictactoe = TicTacToeEnvironment([computed_best_policy, policy])\n",
    "    total_reward = 0.0\n",
    "    while (\n",
    "        tictactoe.get_result() == None\n",
    "        and len([f for f in tictactoe.board if f == None]) > 0\n",
    "    ):\n",
    "        player, field, reward, state = tictactoe.step()\n",
    "        if player == Type.O:\n",
    "            total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def measure_performance(policy, nr_episodes: int = 100):\n",
    "    N = nr_episodes\n",
    "    print(\"statistics over\", N, \"episodes\")\n",
    "    all_rewards = []\n",
    "    for _ in range(N):\n",
    "        episode_reward = run_one_episode(policy)\n",
    "        all_rewards.append(episode_reward)\n",
    "\n",
    "    print(\"mean: {:6.2f}, sigma: {:6.2f}\".format(mean(all_rewards), stdev(all_rewards)))\n",
    "    print()\n",
    "    for n, episode_reward in enumerate(all_rewards[:5], 1):\n",
    "        print(\"ep: {:2d}, total reward: {:5.2f}\".format(n, episode_reward))\n",
    "    print(\"......\")\n",
    "    for n, episode_reward in enumerate(all_rewards[-5:], len(all_rewards) - 5):\n",
    "        print(\"ep: {:2d}, total reward: {:5.2f}\".format(n, episode_reward))\n",
    "\n",
    "\n",
    "measure_performance(random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717bc2cb",
   "metadata": {},
   "source": [
    "# 5. Decisions based on reward agent\n",
    "\n",
    "Next we have the code for the agent that makes the decision based on reward. Since, we are working with a **Markov Decision Process** (MDP), we also have to define a reward function $R(s,a)$ that returns a value based on performing action $a$ in state $s$ both of which have been previously defined.\n",
    "\n",
    "We will define the reward function $R(s,a)$ as follows:\n",
    "- Trivially, if the agent performs some action $â$ that wins the game from $s$, then $R(s,â) = 1$.\n",
    "- If the agent makes a mistake where the wrong action $ã$ loses the game, we then say $R(s,ã) = -1$.\n",
    "- When nothing happens we can simply say $R(s,a)=0$.\n",
    "\n",
    "We will be using Q-learning to find an optimal policy that the agent uses to decide which actions to pick. We will simply denote our policy as the action $a$ that maximizes a function $Q(s,a)$ when the agent is in some state $s$. So, we would have something like:\n",
    "\n",
    "$$a^{best} = \\text{arg}\\max_{a\\in A}Q(s, a)$$\n",
    "\n",
    "For every state, each action has an associated value of $Q$ and we want to pick the $Q$ with the highest value. So, to compute $Q(s,a)$, the agent has to go over each possible pairs of states and actions while getting feedback from the reward function. We will update $Q(s,a)$ iteratively by letting the agent play. We will update $Q$ as follows:\n",
    "\n",
    "$$Q(s,a)^{new} \\leftarrow (1 - \\alpha)\\cdot Q(s,a)+\\alpha\\cdot(R(s,a)+\\gamma\\cdot\\max_{â\\in A}Q(ŝ, â))$$\n",
    "\n",
    "- We perform an action $a$ in the current state $s$.\n",
    "- $\\max_{â\\in A}Q(ŝ, â))$ takes into account future states and returns the largest $Q$, ŝ is the state that is the new state after performing $a$. Then $â$ is the best action.\n",
    "- $\\alpha$ is the learning rate that decides to what extent we overwrite the old value, we will use $\\alpha=0.1$.\n",
    "- The discount factor $\\gamma$ decides how much future rewards should be weighted compared to present rewards at the current time step $t$. We will be using $\\gamma=0.9$.\n",
    "\n",
    "We will find (read: learn) the best values for $Q(s,a)$. This will be done by letting two agents play against each other. To make sure it is balanced and that agents also seek out new options we will introduce a probability $\\epsilon$ that an agent picks a random action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fb4068e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"Q: List[Tuple[List[float], List[Type]]] = []\\n\\n\\ndef get_max_q(current_state: List[Type]) -> float:\\n    \\\"\\\"\\\"Returns the maximum Q value given a state and list of actions (input is hash keys)\\\"\\\"\\\"\\n    max_q = 0.0\\n    for reward, state in Q:\\n        if max(reward) > max_q:\\n            is_iteration = (\\n                len(\\n                    [\\n                        i\\n                        for i in range(len(current_state))\\n                        if current_state[i] == None or current_state[i] == state[i]\\n                    ]\\n                )\\n                == 9\\n            )\\n            if is_iteration:\\n                max_q = max(reward)\\n    return max_q\\n\\n\\ndef update_q(\\n    tictactoe: TicTacToeEnvironment, field: int, \\ud835\\udefc: float = 0.1, \\ud835\\udefe: float = 0.9\\n) -> None:\\n    state = tictactoe.board\\n\\n    reward = 0.5 + (tictactoe.get_reward(field) / 2)\\n    updated_state = [\\n        state[i] if not i == field else tictactoe.get_active()\\n        for i in range(len(state))\\n    ]\\n    max_q = get_max_q(updated_state)\\n    pos = [i for i in range(len(Q)) if Q[i][1] == state]\\n    pos = pos[0] if len(pos) > 0 else None\\n    if not pos == None:\\n        value = (1 - \\ud835\\udefc) * Q[pos][0][field] + \\ud835\\udefc * (reward + \\ud835\\udefe * max_q)\\n        Q[pos][0][field] = value\\n    else:\\n        Q.append(([0.0 if i == field else \\ud835\\udefc * reward for i in range(9)], copy(state)))\\n\\n\\ndef train_random_policy(tictactoe: TicTacToeEnvironment):\\n    available_fields = [\\n        i for i in range(len(tictactoe.board)) if tictactoe.board[i] == None\\n    ]\\n\\n    field = random.choice(available_fields)\\n\\n    update_q(tictactoe, field)\\n\\n    return field\\n\\n\\ndef train(nr_episodes: int = 5000):\\n    N = nr_episodes\\n    for _ in range(N):\\n        tictactoe = TicTacToeEnvironment([computed_best_policy, train_random_policy])\\n        while (\\n            tictactoe.get_result() == None\\n            and len([f for f in tictactoe.board if f == None]) > 0\\n        ):\\n            player, field, reward, state = tictactoe.step()\\n\\n\\ntrain()\";\n",
       "                var nbb_formatted_code = \"Q: List[Tuple[List[float], List[Type]]] = []\\n\\n\\ndef get_max_q(current_state: List[Type]) -> float:\\n    \\\"\\\"\\\"Returns the maximum Q value given a state and list of actions (input is hash keys)\\\"\\\"\\\"\\n    max_q = 0.0\\n    for reward, state in Q:\\n        if max(reward) > max_q:\\n            is_iteration = (\\n                len(\\n                    [\\n                        i\\n                        for i in range(len(current_state))\\n                        if current_state[i] == None or current_state[i] == state[i]\\n                    ]\\n                )\\n                == 9\\n            )\\n            if is_iteration:\\n                max_q = max(reward)\\n    return max_q\\n\\n\\ndef update_q(\\n    tictactoe: TicTacToeEnvironment, field: int, \\ud835\\udefc: float = 0.1, \\ud835\\udefe: float = 0.9\\n) -> None:\\n    state = tictactoe.board\\n\\n    reward = 0.5 + (tictactoe.get_reward(field) / 2)\\n    updated_state = [\\n        state[i] if not i == field else tictactoe.get_active()\\n        for i in range(len(state))\\n    ]\\n    max_q = get_max_q(updated_state)\\n    pos = [i for i in range(len(Q)) if Q[i][1] == state]\\n    pos = pos[0] if len(pos) > 0 else None\\n    if not pos == None:\\n        value = (1 - \\ud835\\udefc) * Q[pos][0][field] + \\ud835\\udefc * (reward + \\ud835\\udefe * max_q)\\n        Q[pos][0][field] = value\\n    else:\\n        Q.append(([0.0 if i == field else \\ud835\\udefc * reward for i in range(9)], copy(state)))\\n\\n\\ndef train_random_policy(tictactoe: TicTacToeEnvironment):\\n    available_fields = [\\n        i for i in range(len(tictactoe.board)) if tictactoe.board[i] == None\\n    ]\\n\\n    field = random.choice(available_fields)\\n\\n    update_q(tictactoe, field)\\n\\n    return field\\n\\n\\ndef train(nr_episodes: int = 5000):\\n    N = nr_episodes\\n    for _ in range(N):\\n        tictactoe = TicTacToeEnvironment([computed_best_policy, train_random_policy])\\n        while (\\n            tictactoe.get_result() == None\\n            and len([f for f in tictactoe.board if f == None]) > 0\\n        ):\\n            player, field, reward, state = tictactoe.step()\\n\\n\\ntrain()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Q: List[Tuple[List[float], List[Type]]] = []\n",
    "\n",
    "\n",
    "def get_max_q(current_state: List[Type]) -> float:\n",
    "    \"\"\"Returns the maximum Q value given a state and list of actions (input is hash keys)\"\"\"\n",
    "    max_q = 0.0\n",
    "    for reward, state in Q:\n",
    "        if max(reward) > max_q:\n",
    "            is_iteration = (\n",
    "                len(\n",
    "                    [\n",
    "                        i\n",
    "                        for i in range(len(current_state))\n",
    "                        if current_state[i] == None or current_state[i] == state[i]\n",
    "                    ]\n",
    "                )\n",
    "                == 9\n",
    "            )\n",
    "            if is_iteration:\n",
    "                max_q = max(reward)\n",
    "    return max_q\n",
    "\n",
    "\n",
    "def update_q(\n",
    "    tictactoe: TicTacToeEnvironment, field: int, 𝛼: float = 0.1, 𝛾: float = 0.9\n",
    ") -> None:\n",
    "    state = tictactoe.board\n",
    "\n",
    "    reward = 0.5 + (tictactoe.get_reward(field) / 2)\n",
    "    updated_state = [\n",
    "        state[i] if not i == field else tictactoe.get_active()\n",
    "        for i in range(len(state))\n",
    "    ]\n",
    "    max_q = get_max_q(updated_state)\n",
    "    pos = [i for i in range(len(Q)) if Q[i][1] == state]\n",
    "    pos = pos[0] if len(pos) > 0 else None\n",
    "    if not pos == None:\n",
    "        value = (1 - 𝛼) * Q[pos][0][field] + 𝛼 * (reward + 𝛾 * max_q)\n",
    "        Q[pos][0][field] = value\n",
    "    else:\n",
    "        Q.append(([0.0 if i == field else 𝛼 * reward for i in range(9)], copy(state)))\n",
    "\n",
    "\n",
    "def train_random_policy(tictactoe: TicTacToeEnvironment):\n",
    "    available_fields = [\n",
    "        i for i in range(len(tictactoe.board)) if tictactoe.board[i] == None\n",
    "    ]\n",
    "\n",
    "    field = random.choice(available_fields)\n",
    "\n",
    "    update_q(tictactoe, field)\n",
    "\n",
    "    return field\n",
    "\n",
    "\n",
    "def train(nr_episodes: int = 5000):\n",
    "    N = nr_episodes\n",
    "    for _ in range(N):\n",
    "        tictactoe = TicTacToeEnvironment([computed_best_policy, train_random_policy])\n",
    "        while (\n",
    "            tictactoe.get_result() == None\n",
    "            and len([f for f in tictactoe.board if f == None]) > 0\n",
    "        ):\n",
    "            player, field, reward, state = tictactoe.step()\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3baf292b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: X 4\tstate: [E, E, E, E, X, E, E, E, E]\n",
      "action: O 6\tstate: [E, E, E, E, X, E, O, E, E], reward: 0.0\n",
      "action: X 0\tstate: [X, E, E, E, X, E, O, E, E]\n",
      "action: O 8\tstate: [X, E, E, E, X, E, O, E, O], reward: 0.0\n",
      "action: X 7\tstate: [X, E, E, E, X, E, O, X, O]\n",
      "action: O 1\tstate: [X, O, E, E, X, E, O, X, O], reward: 0.0\n",
      "action: X 2\tstate: [X, O, X, E, X, E, O, X, O]\n",
      "action: O 3\tstate: [X, O, X, O, X, E, O, X, O], reward: 0.0\n",
      "action: X 5\tstate: [X, O, X, O, X, X, O, X, O]\n",
      "\n",
      "Episode done after 9 steps. Total reward: 0.0\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"def optimal_decision_policy(tictactoe: TicTacToeEnvironment) -> int:\\n    \\\"\\\"\\\"Get best action given a set of possible actions in a given state\\\"\\\"\\\"\\n\\n    actions = [i for i in range(len(tictactoe.board)) if tictactoe.board[i] == None]\\n\\n    # Pick a random action at first\\n    best_action = random.choice(actions)\\n\\n    # Find action with largest Q in given state\\n    max_q = 0\\n    q = [Q[i] for i in range(len(Q)) if Q[i][1] == tictactoe.board]\\n    q = q[0] if len(q) > 0 else None\\n    if not q == None:\\n        for i in range(len(q[0])):\\n            if q[0][i] > max_q:\\n                max_q = q[0][i]\\n                best_action = i\\n\\n    return best_action\\n\\n\\ntotal_reward = 0.0\\n\\ntictactoe = TicTacToeEnvironment([computed_best_policy, optimal_decision_policy])\\nwhile (\\n    tictactoe.get_result() == None\\n    and len([f for f in tictactoe.board if f == None]) > 0\\n):\\n    player, field, reward, state = tictactoe.step()\\n    if player == Type.O:\\n        total_reward += reward\\n        print(\\n            \\\"action: {}\\\\tstate: {}, reward: {}\\\".format(\\n                player.name + \\\" \\\" + str(field), state, reward\\n            )\\n        )\\n    else:\\n        print(\\\"action: {}\\\\tstate: {}\\\".format(player.name + \\\" \\\" + str(field), state))\\n\\nprint(\\n    \\\"\\\\nEpisode done after {} steps. Total reward: {}\\\".format(\\n        tictactoe.get_turns(), total_reward\\n    )\\n)\";\n",
       "                var nbb_formatted_code = \"def optimal_decision_policy(tictactoe: TicTacToeEnvironment) -> int:\\n    \\\"\\\"\\\"Get best action given a set of possible actions in a given state\\\"\\\"\\\"\\n\\n    actions = [i for i in range(len(tictactoe.board)) if tictactoe.board[i] == None]\\n\\n    # Pick a random action at first\\n    best_action = random.choice(actions)\\n\\n    # Find action with largest Q in given state\\n    max_q = 0\\n    q = [Q[i] for i in range(len(Q)) if Q[i][1] == tictactoe.board]\\n    q = q[0] if len(q) > 0 else None\\n    if not q == None:\\n        for i in range(len(q[0])):\\n            if q[0][i] > max_q:\\n                max_q = q[0][i]\\n                best_action = i\\n\\n    return best_action\\n\\n\\ntotal_reward = 0.0\\n\\ntictactoe = TicTacToeEnvironment([computed_best_policy, optimal_decision_policy])\\nwhile (\\n    tictactoe.get_result() == None\\n    and len([f for f in tictactoe.board if f == None]) > 0\\n):\\n    player, field, reward, state = tictactoe.step()\\n    if player == Type.O:\\n        total_reward += reward\\n        print(\\n            \\\"action: {}\\\\tstate: {}, reward: {}\\\".format(\\n                player.name + \\\" \\\" + str(field), state, reward\\n            )\\n        )\\n    else:\\n        print(\\\"action: {}\\\\tstate: {}\\\".format(player.name + \\\" \\\" + str(field), state))\\n\\nprint(\\n    \\\"\\\\nEpisode done after {} steps. Total reward: {}\\\".format(\\n        tictactoe.get_turns(), total_reward\\n    )\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def optimal_decision_policy(tictactoe: TicTacToeEnvironment) -> int:\n",
    "    \"\"\"Get best action given a set of possible actions in a given state\"\"\"\n",
    "\n",
    "    actions = [i for i in range(len(tictactoe.board)) if tictactoe.board[i] == None]\n",
    "\n",
    "    # Pick a random action at first\n",
    "    best_action = random.choice(actions)\n",
    "\n",
    "    # Find action with largest Q in given state\n",
    "    max_q = 0\n",
    "    q = [Q[i] for i in range(len(Q)) if Q[i][1] == tictactoe.board]\n",
    "    q = q[0] if len(q) > 0 else None\n",
    "    if not q == None:\n",
    "        for i in range(len(q[0])):\n",
    "            if q[0][i] > max_q:\n",
    "                max_q = q[0][i]\n",
    "                best_action = i\n",
    "\n",
    "    return best_action\n",
    "\n",
    "\n",
    "total_reward = 0.0\n",
    "\n",
    "tictactoe = TicTacToeEnvironment([computed_best_policy, optimal_decision_policy])\n",
    "while (\n",
    "    tictactoe.get_result() == None\n",
    "    and len([f for f in tictactoe.board if f == None]) > 0\n",
    "):\n",
    "    player, field, reward, state = tictactoe.step()\n",
    "    if player == Type.O:\n",
    "        total_reward += reward\n",
    "        print(\n",
    "            \"action: {}\\tstate: {}, reward: {}\".format(\n",
    "                player.name + \" \" + str(field), state, reward\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\"action: {}\\tstate: {}\".format(player.name + \" \" + str(field), state))\n",
    "\n",
    "print(\n",
    "    \"\\nEpisode done after {} steps. Total reward: {}\".format(\n",
    "        tictactoe.get_turns(), total_reward\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cef22c",
   "metadata": {},
   "source": [
    "Note that reward is always $0.0$, since the best computed policy and optimal decision policy are against each other. The best possible result for both is a tie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8201d8",
   "metadata": {},
   "source": [
    "# Value iteration vs Q-learning\n",
    "\n",
    "## How does value iteration work\n",
    "Let's dive a bit deeper into what value iteration is and the difference with Q-learning. For the value iteration reinforcement learning algorithm the agent knows two things before taking a specific action.\n",
    "\n",
    "1. The probabilities of ending up in other new states given that specific action from the current state. Basically, the agent knows a **transition function**. So, we can formally define such a function as follows: A transition function $T(s,a,s')$ that gives the probability of moving from a state $s$ to $s'$ when performing action $a$.\n",
    "2. The immediate reward that would be received for taking that action $a$ from the current state $s$ to $s'$. This is that reward function $R(s,a)$.\n",
    "\n",
    "For value iteration we can say that the agent is able to look ahead and calculate which route to take based on transition and reward. The algorithm will take an expression known as an **experience tuple** to keep track of data that changes with each state and action. Since, value iteration is a model based approach, it builds a model of the transition function $T(s,a,s')$ and reward function $R(s,a)$ using the experience tuples. Then once the models are built we can use value iteration to determine the optimal values of each state. The optimal values of each state s are based on the action a that generates the best expected cumulative reward. So, we calculate the value of each state-action pair in the training phase, then at each time-step, we select the action where that state-action pair value is the highest ($max_aQ^*(s,a)$).\n",
    "\n",
    "## The difference\n",
    "So, in most cases it is not efficient to first build an complete model (again that means the transition and reward function) of the environment before you start. For two reasons: the model might take a long time to create and second, the environment might be unpredictable or always changing. You would want for the agent to able to adapt to that. So, Q-learning would be a model-free reinforcement learning algorithm. The agent has to interact with the environment and learn using trial-and-error. It cannot look ahead. In Q-learning the agent builds a table of **Q-values** denoted as $Q(s,a)$ as it takes an action $a$ from a state $s$ and the receives a reward $r$. The agent uses the table to see what action is best based on previous experiences (or outcomes) of taking some action.\n",
    "\n",
    "## Policy iteration\n",
    "Next, we also have policy iteration which is a way of finding the optimal policy given a set of states and actions. Let's assume we some policy $(\\pi:S\\rightarrow A)$ that assigns an action to each state. Action $\\pi(s)$ will be chosen each time the system is a state s.\n",
    "\n",
    "1. Evaluate a given policy (we can initialize a policy arbitrarily for all states ($s\\in S$) by calculating a value function for all states $s\\in S$ under a given policy. So the value function would be the expected reward collected at the first step, plus the expected discounted value at the next state. $$V_{\\pi}(s) = E[R(s, \\pi(s), s') + \\gamma V(s')]$$.\n",
    "\n",
    "2. Then we improve the policy by finding a better action for every state $s$ (we give each iteration a subscript, here 1). $$\\pi_1(s)=\\text{arg}\\max_{a\\in A}E[R(s, a, s') + \\gamma V(s')]$$\n",
    "\n",
    "3. We repeat these steps until we converge at an optimal value function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b495613",
   "metadata": {},
   "source": [
    "# Sources\n",
    "- For reinforcement learning (Q-learning): https://towardsdatascience.com/how-to-play-tic-tac-toe-using-reinforcement-learning-9604130e56f6\n",
    "- Value iteration vs. Q-learning algorithms: https://automaticaddison.com/value-iteration-vs-q-learning-algorithm-in-python-step-by-step/\n",
    "- Policy iteration: https://medium.com/@pesupavish/policy-iteration-easy-example-d3fd1eb98c6c"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21cebe2f5e3224dba0c406142577afe24fece8cdabfe36ea782fc3cbe70ab6c4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
